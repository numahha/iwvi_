{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9360249",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--env-name ENV_NAME] [--policy POLICY]\n",
      "                             [--eval EVAL] [--gamma G] [--tau G] [--lr G]\n",
      "                             [--alpha G] [--automatic_entropy_tuning G]\n",
      "                             [--seed N] [--batch_size N] [--num_steps N]\n",
      "                             [--hidden_size N] [--updates_per_step N]\n",
      "                             [--start_steps N] [--target_update_interval N]\n",
      "                             [--replay_size N] [--cuda]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\toru.hishinuma\\AppData\\Roaming\\jupyter\\runtime\\kernel-d432507d-577c-413c-b6ba-778376897676.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toru.hishinuma\\anaconda3\\envs\\note\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3449: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import gym\n",
    "import numpy as np\n",
    "import itertools\n",
    "import torch\n",
    "from sac import SAC\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from replay_memory import ReplayMemory\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch Soft Actor-Critic Args')\n",
    "# parser.add_argument('--env-name', default=\"HalfCheetah-v2\",\n",
    "parser.add_argument('--env-name', default=\"Pendulum-v2\",\n",
    "                    help='Mujoco Gym environment (default: HalfCheetah-v2)')\n",
    "parser.add_argument('--policy', default=\"Gaussian\",\n",
    "                    help='Policy Type: Gaussian | Deterministic (default: Gaussian)')\n",
    "parser.add_argument('--eval', type=bool, default=True,\n",
    "                    help='Evaluates a policy a policy every 10 episode (default: True)')\n",
    "parser.add_argument('--gamma', type=float, default=0.99, metavar='G',\n",
    "                    help='discount factor for reward (default: 0.99)')\n",
    "parser.add_argument('--tau', type=float, default=0.005, metavar='G',\n",
    "                    help='target smoothing coefficient(τ) (default: 0.005)')\n",
    "parser.add_argument('--lr', type=float, default=0.0003, metavar='G',\n",
    "                    help='learning rate (default: 0.0003)')\n",
    "parser.add_argument('--alpha', type=float, default=0.2, metavar='G',\n",
    "                    help='Temperature parameter α determines the relative importance of the entropy\\\n",
    "                            term against the reward (default: 0.2)')\n",
    "parser.add_argument('--automatic_entropy_tuning', type=bool, default=False, metavar='G',\n",
    "                    help='Automaically adjust α (default: False)')\n",
    "parser.add_argument('--seed', type=int, default=123456, metavar='N',\n",
    "                    help='random seed (default: 123456)')\n",
    "parser.add_argument('--batch_size', type=int, default=256, metavar='N',\n",
    "                    help='batch size (default: 256)')\n",
    "parser.add_argument('--num_steps', type=int, default=1000001, metavar='N',\n",
    "                    help='maximum number of steps (default: 1000000)')\n",
    "parser.add_argument('--hidden_size', type=int, default=256, metavar='N',\n",
    "                    help='hidden size (default: 256)')\n",
    "parser.add_argument('--updates_per_step', type=int, default=1, metavar='N',\n",
    "                    help='model updates per simulator step (default: 1)')\n",
    "parser.add_argument('--start_steps', type=int, default=10000, metavar='N',\n",
    "                    help='Steps sampling random actions (default: 10000)')\n",
    "parser.add_argument('--target_update_interval', type=int, default=1, metavar='N',\n",
    "                    help='Value target update per no. of updates per step (default: 1)')\n",
    "parser.add_argument('--replay_size', type=int, default=1000000, metavar='N',\n",
    "                    help='size of replay buffer (default: 10000000)')\n",
    "parser.add_argument('--cuda', action=\"store_true\",\n",
    "                    help='run on CUDA (default: False)')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Environment\n",
    "# env = NormalizedActions(gym.make(args.env_name))\n",
    "env = gym.make(args.env_name)\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "env.seed(args.seed)\n",
    "\n",
    "# Agent\n",
    "agent = SAC(env.observation_space.shape[0], env.action_space, args)\n",
    "\n",
    "#Tesnorboard\n",
    "writer = SummaryWriter('runs/{}_SAC_{}_{}_{}'.format(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"), args.env_name,\n",
    "                                                             args.policy, \"autotune\" if args.automatic_entropy_tuning else \"\"))\n",
    "\n",
    "# Memory\n",
    "memory = ReplayMemory(args.replay_size)\n",
    "\n",
    "# Training Loop\n",
    "total_numsteps = 0\n",
    "updates = 0\n",
    "\n",
    "for i_episode in itertools.count(1):\n",
    "    episode_reward = 0\n",
    "    episode_steps = 0\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        if args.start_steps > total_numsteps:\n",
    "            action = env.action_space.sample()  # Sample random action\n",
    "        else:\n",
    "            action = agent.select_action(state)  # Sample action from policy\n",
    "\n",
    "        if len(memory) > args.batch_size:\n",
    "            # Number of updates per step in environment\n",
    "            for i in range(args.updates_per_step):\n",
    "                # Update parameters of all the networks\n",
    "                critic_1_loss, critic_2_loss, policy_loss, ent_loss, alpha = agent.update_parameters(memory, args.batch_size, updates)\n",
    "\n",
    "                writer.add_scalar('loss/critic_1', critic_1_loss, updates)\n",
    "                writer.add_scalar('loss/critic_2', critic_2_loss, updates)\n",
    "                writer.add_scalar('loss/policy', policy_loss, updates)\n",
    "                writer.add_scalar('loss/entropy_loss', ent_loss, updates)\n",
    "                writer.add_scalar('entropy_temprature/alpha', alpha, updates)\n",
    "                updates += 1\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action) # Step\n",
    "        episode_steps += 1\n",
    "        total_numsteps += 1\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Ignore the \"done\" signal if it comes from hitting the time horizon.\n",
    "        # (https://github.com/openai/spinningup/blob/master/spinup/algos/sac/sac.py)\n",
    "        mask = 1 if episode_steps == env._max_episode_steps else float(not done)\n",
    "\n",
    "        memory.push(state, action, reward, next_state, mask) # Append transition to memory\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    if total_numsteps > args.num_steps:\n",
    "        break\n",
    "\n",
    "    writer.add_scalar('reward/train', episode_reward, i_episode)\n",
    "    print(\"Episode: {}, total numsteps: {}, episode steps: {}, reward: {}\".format(i_episode, total_numsteps, episode_steps, round(episode_reward, 2)))\n",
    "\n",
    "    if i_episode % 10 == 0 and args.eval is True:\n",
    "        avg_reward = 0.\n",
    "        episodes = 10\n",
    "        for _  in range(episodes):\n",
    "            state = env.reset()\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = agent.select_action(state, evaluate=True)\n",
    "\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                episode_reward += reward\n",
    "\n",
    "\n",
    "                state = next_state\n",
    "            avg_reward += episode_reward\n",
    "        avg_reward /= episodes\n",
    "\n",
    "\n",
    "        writer.add_scalar('avg_reward/test', avg_reward, i_episode)\n",
    "\n",
    "        print(\"----------------------------------------\")\n",
    "        print(\"Test Episodes: {}, Avg. Reward: {}\".format(episodes, round(avg_reward, 2)))\n",
    "        print(\"----------------------------------------\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6f69f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
